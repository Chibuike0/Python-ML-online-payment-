# -*- coding: utf-8 -*-
"""BDM2503_GROUP PROJECT-ONLINE PAYMENT FRAUD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LS-AB_YJ6u6WrTs6Nlc2PhdbnU12TDZZ

An Online fraud ML project by Kyrian Okoroama.

The idea was to understand the dataset and use appropriate ML algorithm to build a model that can detect when an online transaction is fraud.

THe take away here is that I had to learn how to clean the dataset, data preprocessing and perfrom feature engineering, making sure that the correlated independent variables to target variable is retained for ML and so much more.
"""

# IMPORTING RELEVANT LIBRARIES

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

"""The below column reference:

**Step**: represents a unit of time where 1 step equals 1 hour

**Type**: type of online transaction

amount: the amount of the transaction

**NameOrig**: customer starting the transaction

**OldbalanceOrg** balance before the transaction

**NewbalanceOrig**: balance after the transaction

**NameDest**: recipient of the transaction

**OldbalanceDest**: initial balance of recipient before the transaction

**NewbalanceDest**: the new balance of recipient after the transaction

**isFraud**: fraud transaction (Target Variable)

https://www.kaggle.com/datasets/rupakroy/online-payments-fraud-detection-dataset
"""

# excel data download allowd to edit to save in laptop
df = pd.read_csv('/content/Online_fraud_detection_data-kaggle-.csv')
df.head(5)

df.describe().round(2)

df.info()

df.isnull().sum()

#df = df.dropna()

df.shape

#df.isnull().sum()

# df = df.drop(['step', 'nameOrig','nameDest'], axis=1)

# data imbalance
# Create the countplot
sns.countplot(x='isFraud', data=df)

# Calculate percentages
total = len(df)
fraud_count = df['isFraud'].sum()
non_fraud_count = total - fraud_count

fraud_percentage = (fraud_count / total) * 100
non_fraud_percentage = (non_fraud_count / total) * 100

# plot with percentages of Fraud and not Fraud in the dataset
plt.text(1, fraud_count, f'{fraud_percentage:.2f}%', ha='center', va='bottom', fontsize=12)
plt.text(0, non_fraud_count, f'{non_fraud_percentage:.2f}%', ha='center', va='bottom', fontsize=12)

plt.show()

plt.figure(figsize=(10, 6))  # Set the figure size
sns.countplot(data=df, x='type', palette='viridis')  # Create the barplot
plt.title('Frequency of Type')
plt.xlabel('Type')
plt.ylabel('Frequency')

# Show the plot
plt.show()

plt.figure(figsize=(10, 6))  # Set the figure size

# Make sure the column name matches the one in your DataFrame
sns.barplot( x='type', y='amount', data=df)  # Create the barplot

plt.title('Type of transfer amount')
plt.xlabel('Type')  # Correct the capitalization
plt.ylabel('Amount')

# Show the plot
plt.show()

df_cross = pd.crosstab(index=df['type'], columns=df['isFraud'])
df_cross

# Define a dictionary to map categorical values to numerical values. Due to time constrain, a dictionary of 8 instances where done
label_map = {'PAYMENT': 1, 'TRANSFER': 2, 'CASH_OUT': 3, 'CASH_IN': 4, 'DEBIT': 5}

# Map the 'Team' column to numerical values
df['Type_label'] = df['type'].map(label_map)

df = df.drop(['step','type', 'isFlaggedFraud'], axis=1)

df.head()

correl = df.corr()
print(correl['isFraud'].sort_values(ascending=False))

plt.figure(figsize=(12,10))
sns.heatmap(df.corr(), annot=True)

# SMOTE was used to handle oversampling and split the data

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# prepare column for modeling
X = df[['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'Type_label']]
y = df[['isFraud']]

df.shape

# Split the dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# HANDLING DATA IMBALANCE
# Apply SMOTE (sythentic minority oversampling Technique) for oversampling the minority class
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# check for x train and test shape
X_resampled.shape, X_test.shape

df_resample = pd.concat([X_resampled, y_resampled])

# check data imbalance
# Create the countplot
sns.countplot(x='isFraud', data=df_resample)

# Calculate percentages
total = len(df_resample)
fraud_count = df_resample['isFraud'].sum()
non_fraud_count = total - fraud_count

fraud_percentage = (fraud_count / total) * 100
non_fraud_percentage = (non_fraud_count / total) * 100

# Plot with percentages of Fraud and not Fraud in the dataset
plt.text(0, fraud_count, f'{fraud_percentage:.2f}%', fontsize=12, ha='center')
plt.text(1, non_fraud_count, f'{non_fraud_percentage:.2f}%', fontsize=12, ha='center')

plt.show()

#from sklearn.preprocessing import StandardScaler

# scale the split data
#scaler=StandardScaler()
#X_train_sc=scaler.fit_transform(X_train)
#X_test_sc = scaler.transform(X_test)

pd.DataFrame(X_resampled).head()

#pd.DataFrame(X_train_sc).head()

# machine learning
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
#from sklearn.naive_bayes import GaussianNB
#from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
# accuracy
from sklearn.metrics import accuracy_score
# Tree visuals
from sklearn.tree import plot_tree
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import classification_report, balanced_accuracy_score
from sklearn.metrics import classification_report, confusion_matrix

# fit logistic regression
lr=LogisticRegression()

lr.fit(X_resampled,y_resampled)
Y_pred = lr.predict(X_test)

print(classification_report(y_test, Y_pred))
print('*'*100)
pd.DataFrame(confusion_matrix(y_test,Y_pred),\
            columns=["Predicted Not-Fraud", "Predicted isFraud"],\
            index=["Not-Fraud","isFraud"] )

# Create a KNN classifier with a specified number of neighbors (e.g., 5)
knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors as needed

# Fit the KNN classifier on the training data
knn.fit(X_resampled, y_resampled)

# Make predictions on the test data
y_pred_knn = knn.predict(X_test)

# Calculate the accuracy of the model
acc_knn = round(accuracy_score(y_test, y_pred_knn)*100, 2)
print("KNN Accuracy:", acc_knn)

print(classification_report(y_test, y_pred_knn))
print('*'*100)
pd.DataFrame(confusion_matrix(y_test,y_pred_knn),\
            columns=["Predicted Not-Fraud", "Predicted isFraud"],\
            index=["Not-Fraud","isFraud"] )

# Create a Decision Tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42, max_depth= 5)  # You can set other parameters as needed

# Fit the Decision Tree classifier on the training data
dt_classifier.fit(X_resampled, y_resampled)

# Make predictions on the test data
y_pred_dt = dt_classifier.predict(X_test)

# Calculate the accuracy of the model
acc_dt = round(accuracy_score(y_test, y_pred_dt)* 100, 2)
print("Decision Tree Accuracy:", acc_dt)

print(classification_report(y_test, y_pred_dt))
print('*'*100)
pd.DataFrame(confusion_matrix(y_test,y_pred_dt),\
            columns=["Predicted Not-Fraud", "Predicted isFraud"],\
            index=["Not-Fraud","isFraud"] )

# Make predictions on the test data
y_pred_dt = dt_classifier.predict(X_test)

# Calculate the accuracy of the model
acc_dt = round(accuracy_score(y_test, y_pred_dt)* 100, 2)
print("Decision Tree Accuracy:", acc_dt)

# Visualize the decision tree
plt.figure(figsize=(20, 10))  # Adjust the figure size as needed
plot_tree(dt_classifier, filled=True, feature_names=X.columns)  # Use your feature names instead of X.columns
plt.show()

# Create a Random Forest classifier with fewer trees
rf_classifier = RandomForestClassifier(n_estimators=3, random_state=42)

# Fit the Random Forest classifier on the training data
rf_classifier.fit(X_resampled, y_resampled)

# Access one of the decision trees from the Random Forest (e.g., the first tree)
tree_estimator = rf_classifier.estimators_[1]  # Change the index to access a different tree if needed

# Make predictions on the test data
y_pred_rf = tree_estimator.predict(X_test)  # You can also use the original X_test

# Calculate the accuracy of the model
acc_rf = round(accuracy_score(y_test, y_pred_rf) * 100, 2)
print("Random Forest Tree Accuracy:", acc_rf)



print(classification_report(y_test, y_pred_rf))
print('*'*100)
pd.DataFrame(confusion_matrix(y_test,y_pred_rf),\
            columns=["Predicted Not-Fraud", "Predicted isFraud"],\
            index=["Not-Fraud","isFraud"] )

# Visualize the decision tree
plt.figure(figsize=(12, 8))  # Adjust the figure size as needed
plot_tree(tree_estimator, filled=True, feature_names=X.columns)  # Use your feature names instead of X.columns
plt.title("Random Forest Tree")
plt.show()

"""**DISCUSSION**

Based on the different ML models used for this project, we deduced that Random Forest classifier had better accuracy 99.67%, performed better in terms of Harmonic mean, determing better instances for Fraud than other models (logistics, Tress classification, KNN).

On the other hand all models on prediction of not Fraud (0) were good but different minimal for isFraud (1). Therefore Precision, Recall and F1 scores were lower when compared to Random Forest.

Overall the models had more than 90% accuracy on predicting Fraud, the least of accuracy is Logistic regression 93%, KNN was 97%, Tree classification at 99% and Random Forest at 99.67%. Again Random Forest stood out among the rest due its ability to handle better Precision, Recall and F1 scores on Fraud (1)
"""